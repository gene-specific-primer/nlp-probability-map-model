{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"D:\\\\opt\\\\kaggle-nlp\"\n",
    "# 载入数据集\n",
    "train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"8196_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I dont know why people think this is such a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This movie could have been very good, but com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"10633_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I watched this video at a friend's house. I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A friend of mine bought this film for £1, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"2486_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What happens when an army of wetbacks, towelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"6811_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Although I generally do not like remakes beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"11744_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"7369_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I had a feeling that after \\\"Submerged\\\", thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"12081_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"note to George Litman, and others: the Myster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"3561_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Stephen King adaptation (scripted by King him...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"4489_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"`The Matrix' was an exciting summer blockbust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"3951_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Ulli Lommel's 1980 film 'The Boogey Man' is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"3304_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This movie is one among the very few Indian m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"9352_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Most people, especially young people, may not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"3374_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"Soylent Green\\\" is one of the best and most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"10782_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Michael Stearns plays Mike, a sexually frustr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"5414_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This happy-go-luck 1939 military swashbuckler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"10492_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I would love to have that two hours of my lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"3350_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The script for this movie was probably found ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"6581_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Looking for Quo Vadis at my local video store...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"2203_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Note to all mad scientists everywhere: if you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"689_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What the ........... is this ? This must, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"9152_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Intrigued by the synopsis (every gay video th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"6077_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Would anyone really watch this RUBBISH if it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>\"9389_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Red Rock West (1993)&lt;br /&gt;&lt;br /&gt;Nicolas Cage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>\"9251_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"what can i say?, ms Erika Eleniak is my favor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>\"1422_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The spoiler warning is for those people who w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>\"7415_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"What do you call a horror story without horro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>\"7492_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Though not a horror film in the traditional s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>\"7689_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This was what black society was like before t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>\"12370_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"They probably should have called this movie T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>\"5625_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Attractive Marjorie(Farrah Fawcett)lives in f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>\"9397_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Vaguely reminiscent of great 1940's westerns,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>\"5992_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I admit I had no idea what to expect before v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>\"2488_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"To me, the final scene, in which Harris respo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>\"9627_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"This is by far the funniest short made by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>\"3822_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"To be a Buster Keaton fan is to have your hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>\"5983_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I was one of those \\\"few Americans\\\" that gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>\"8021_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Visually disjointed and full of itself, the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>\"3471_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"this movie had more holes than a piece of swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>\"6034_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Last November, I had a chance to see this fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>\"1988_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"First off, I'd like to make a correction on a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>\"7623_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"While originally reluctant to jump on the ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>\"5974_7\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I heard about this movie when watching VH1's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>\"2034_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I've never been huge on IMAX films. They're c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>\"9416_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Steve McQueen has certainly a lot of loyal fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>\"10994_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Sometimes you wonder how some people get fund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>\"10957_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I am a student of film, and have been for sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>\"2372_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Unimaginably stupid, redundant and humiliatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>\"3453_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It seems like more consideration has gone int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>\"5064_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"I don't believe they made this film. Complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>\"10905_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Guy is a loser. Can't get girls, needs to bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>\"10194_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"This 30 minute documentary Buñuel made in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>\"8478_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"I saw this movie as a child and it broke my h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  sentiment                                             review\n",
       "0       \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1       \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2       \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3       \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4       \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n",
       "5       \"8196_8\"          1  \"I dont know why people think this is such a b...\n",
       "6       \"7166_2\"          0  \"This movie could have been very good, but com...\n",
       "7      \"10633_1\"          0  \"I watched this video at a friend's house. I'm...\n",
       "8        \"319_1\"          0  \"A friend of mine bought this film for £1, and...\n",
       "9      \"8713_10\"          1  \"<br /><br />This movie is full of references....\n",
       "10      \"2486_3\"          0  \"What happens when an army of wetbacks, towelh...\n",
       "11     \"6811_10\"          1  \"Although I generally do not like remakes beli...\n",
       "12     \"11744_9\"          1  \"\\\"Mr. Harvey Lights a Candle\\\" is anchored by...\n",
       "13      \"7369_1\"          0  \"I had a feeling that after \\\"Submerged\\\", thi...\n",
       "14     \"12081_1\"          0  \"note to George Litman, and others: the Myster...\n",
       "15      \"3561_4\"          0  \"Stephen King adaptation (scripted by King him...\n",
       "16      \"4489_1\"          0  \"`The Matrix' was an exciting summer blockbust...\n",
       "17      \"3951_2\"          0  \"Ulli Lommel's 1980 film 'The Boogey Man' is n...\n",
       "18     \"3304_10\"          1  \"This movie is one among the very few Indian m...\n",
       "19     \"9352_10\"          1  \"Most people, especially young people, may not...\n",
       "20      \"3374_7\"          1  \"\\\"Soylent Green\\\" is one of the best and most...\n",
       "21     \"10782_7\"          1  \"Michael Stearns plays Mike, a sexually frustr...\n",
       "22     \"5414_10\"          1  \"This happy-go-luck 1939 military swashbuckler...\n",
       "23     \"10492_1\"          0  \"I would love to have that two hours of my lif...\n",
       "24      \"3350_3\"          0  \"The script for this movie was probably found ...\n",
       "25      \"6581_7\"          1  \"Looking for Quo Vadis at my local video store...\n",
       "26      \"2203_3\"          0  \"Note to all mad scientists everywhere: if you...\n",
       "27       \"689_1\"          0  \"What the ........... is this ? This must, wit...\n",
       "28      \"9152_1\"          0  \"Intrigued by the synopsis (every gay video th...\n",
       "29      \"6077_1\"          0  \"Would anyone really watch this RUBBISH if it ...\n",
       "...          ...        ...                                                ...\n",
       "24970   \"9389_7\"          1  \"Red Rock West (1993)<br /><br />Nicolas Cage ...\n",
       "24971   \"9251_9\"          1  \"what can i say?, ms Erika Eleniak is my favor...\n",
       "24972  \"1422_10\"          1  \"The spoiler warning is for those people who w...\n",
       "24973   \"7415_2\"          0  \"What do you call a horror story without horro...\n",
       "24974   \"7492_7\"          1  \"Though not a horror film in the traditional s...\n",
       "24975  \"7689_10\"          1  \"This was what black society was like before t...\n",
       "24976  \"12370_4\"          0  \"They probably should have called this movie T...\n",
       "24977   \"5625_8\"          1  \"Attractive Marjorie(Farrah Fawcett)lives in f...\n",
       "24978   \"9397_9\"          1  \"Vaguely reminiscent of great 1940's westerns,...\n",
       "24979   \"5992_7\"          1  \"I admit I had no idea what to expect before v...\n",
       "24980  \"2488_10\"          1  \"To me, the final scene, in which Harris respo...\n",
       "24981  \"9627_10\"          1  \"This is by far the funniest short made by the...\n",
       "24982   \"3822_2\"          0  \"To be a Buster Keaton fan is to have your hea...\n",
       "24983   \"5983_4\"          0  \"I was one of those \\\"few Americans\\\" that gre...\n",
       "24984   \"8021_2\"          0  \"Visually disjointed and full of itself, the d...\n",
       "24985   \"3471_3\"          0  \"this movie had more holes than a piece of swi...\n",
       "24986  \"6034_10\"          1  \"Last November, I had a chance to see this fil...\n",
       "24987   \"1988_9\"          1  \"First off, I'd like to make a correction on a...\n",
       "24988   \"7623_9\"          1  \"While originally reluctant to jump on the ban...\n",
       "24989   \"5974_7\"          1  \"I heard about this movie when watching VH1's ...\n",
       "24990   \"2034_9\"          1  \"I've never been huge on IMAX films. They're c...\n",
       "24991   \"9416_3\"          0  \"Steve McQueen has certainly a lot of loyal fa...\n",
       "24992  \"10994_1\"          0  \"Sometimes you wonder how some people get fund...\n",
       "24993  \"10957_3\"          0  \"I am a student of film, and have been for sev...\n",
       "24994   \"2372_1\"          0  \"Unimaginably stupid, redundant and humiliatin...\n",
       "24995   \"3453_3\"          0  \"It seems like more consideration has gone int...\n",
       "24996   \"5064_1\"          0  \"I don't believe they made this film. Complete...\n",
       "24997  \"10905_3\"          0  \"Guy is a loser. Can't get girls, needs to bui...\n",
       "24998  \"10194_3\"          0  \"This 30 minute documentary Buñuel made in the...\n",
       "24999   \"8478_8\"          1  \"I saw this movie as a child and it broke my h...\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "====================================================================================================\n",
      "['id' 'sentiment' 'review']\n",
      "====================================================================================================\n",
      "         id  sentiment                                             review\n",
      "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
      "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
      "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
      "====================================================================================================\n",
      "           id                                             review\n",
      "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
      "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
      "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(\"=\"*100)\n",
    "print(train.columns.values)\n",
    "print(\"=\"*100)\n",
    "print(train.head(3))\n",
    "print(\"=\"*100)\n",
    "print(test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    '''\n",
    "    把IMDB的评论转成词序列\n",
    "    参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613\n",
    "    '''\n",
    "    # 去掉HTML标签，拿到内容\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    # 用正则表达式取出符合规范的部分\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # 小写化所有的词，并转成词list\n",
    "    words = review_text.lower().split()\n",
    "    # 返回words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with all this stuff going down at the moment with mj i ve started listening to his music watching the odd documentary here and there watched the wiz and watched moonwalker again maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent moonwalker is part biography part feature film which i remember going to see at the cinema when it was originally released some of it has subtle messages about mj s feeling towards the press and also the obvious message of drugs are bad m kay visually impressive but of course this is all about michael jackson so unless you remotely like mj in anyway then you are going to hate this and find it boring some may call mj an egotist for consenting to the making of this movie but mj and most of his fans would say that he made it for the fans which if true is really nice of him the actual feature film bit when it finally starts is only on for minutes or so excluding the smooth criminal sequence and joe pesci is convincing as a psychopathic all powerful drug lord why he wants mj dead so bad is beyond me because mj overheard his plans nah joe pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno maybe he just hates mj s music lots of cool things in this like mj turning into a car and a robot and the whole speed demon sequence also the director must have had the patience of a saint when it came to filming the kiddy bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene bottom line this movie is for people who like mj on one level or another which i think is most people if not then stay away it does try and give off a wholesome message and ironically mj s bestest buddy in this movie is a girl michael jackson is truly one of the most talented people ever to grace this planet but is he guilty well with all the attention i ve gave this subject hmmm well i don t know because people can be different behind closed doors i know this for a fact he is either an extremely nice but stupid guy or one of the most sickest liars i hope he is not the latter \n",
      "\n",
      "naturally in a film who s main themes are of mortality nostalgia and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones however there is a craftsmanship and completeness to the film which anyone can enjoy the pace is steady and constant the characters full and engaging the relationships and interactions natural showing that you do not need floods of tears to show emotion screams to show fear shouting to show dispute or violence to show anger naturally joyce s short story lends the film a ready made structure as perfect as a polished diamond but the small changes huston makes such as the inclusion of the poem fit in neatly it is truly a masterpiece of tact subtlety and overwhelming beauty\n"
     ]
    }
   ],
   "source": [
    "# 预处理数据\n",
    "label = train['sentiment']\n",
    "train_data = []\n",
    "for i in range(len(train['review'])):\n",
    "    train_data.append(' '.join(review_to_wordlist(train['review'][i])))#append只增加一个索引位\n",
    "test_data = []\n",
    "for i in range(len(test['review'])):\n",
    "    test_data.append(' '.join(review_to_wordlist(test['review'][i])))\n",
    "\n",
    "# 预览数据\n",
    "print(train_data[0], '\\n')\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征处理\n",
    "kaggle-nlp 采用BOW方法处理了特征，下面我们采用TF-IDF向量、Word2vec向量做特征处理\n",
    "## 1.TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF处理结束.\n",
      "train: \n",
      " (1, 810866)\n",
      "test: \n",
      " (1, 810866)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "\n",
    "\"\"\"\n",
    "min_df: 最小支持度为2（词汇出现的最小次数）\n",
    "max_features: 默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集\n",
    "strip_accents: 将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号\n",
    "analyzer: 设置返回类型\n",
    "token_pattern: 表示token的正则表达式，需要设置analyzer == 'word'，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作token\n",
    "ngram_range: 词组切分的长度范围\n",
    "use_idf: 启用逆文档频率重新加权\n",
    "use_idf：默认为True，权值是tf*idf，如果设为False，将不使用idf，就是只使用tf，相当于CountVectorizer了。\n",
    "smooth_idf: idf平滑参数，默认为True，idf=ln((文档总数+1)/(包含该词的文档数+1))+1，如果设为False，idf=ln(文档总数/包含该词的文档数)+1\n",
    "sublinear_tf: 默认为False，如果设为True，则替换tf为1 + log(tf)\n",
    "stop_words: 设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表\n",
    "\"\"\"\n",
    "tfidf = TFIDF(min_df=2,\n",
    "           max_features=None,\n",
    "           strip_accents='unicode',\n",
    "           analyzer='word',\n",
    "           token_pattern=r'\\w{1,}',#匹配\\w 1次以上\n",
    "           ngram_range=(1, 3),  # 二元文法模型\n",
    "           use_idf=1,\n",
    "           smooth_idf=1,\n",
    "           sublinear_tf=1,\n",
    "           stop_words = 'english') # 去掉英文停用词\n",
    "\n",
    "# 合并训练和测试集以便进行TFIDF向量化操作\n",
    "data_all = train_data + test_data\n",
    "len_train = len(train_data)\n",
    "\n",
    "tfidf.fit(data_all)\n",
    "data_all = tfidf.transform(data_all)\n",
    "\n",
    "# 恢复成训练集和测试集部分\n",
    "train_x = data_all[:len_train]\n",
    "test_x = data_all[len_train:]\n",
    "\n",
    "print('TF-IDF处理结束.')\n",
    "\n",
    "print(\"train: \\n\", np.shape(train_x[0]))\n",
    "print(\"test: \\n\", np.shape(test_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多项式贝叶斯分类器10折交叉验证得分:  \n",
      " [ 0.95134592  0.94728448  0.951648    0.94707712  0.95122816  0.94939968\n",
      "  0.95240704  0.95434432  0.94438528  0.94930816]\n",
      "\n",
      "多项式贝叶斯分类器10折交叉验证得分:  0.949842816\n"
     ]
    }
   ],
   "source": [
    "# 朴素贝叶斯训练\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "model_NB = MNB() # (alpha=1.0, class_prior=None, fit_prior=True)\n",
    "# 为了在预测的时候使用\n",
    "model_NB.fit(train_x, label)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"多项式贝叶斯分类器10折交叉验证得分:  \\n\", cross_val_score(model_NB, train_x, label, cv=10, scoring='roc_auc'))\n",
    "print(\"\\n多项式贝叶斯分类器10折交叉验证得分: \", np.mean(cross_val_score(model_NB, train_x, label, cv=10, scoring='roc_auc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predicted = np.array(model_NB.predict(test_x))\n",
    "# print('保存结果...')\n",
    "\n",
    "# submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "# print(submission_df.head(10))\n",
    "# submission_df.to_csv('/Users/jiangzl/Desktop/submission_br.csv',columns = ['id','sentiment'], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # 设定grid search的参数\n",
    "# grid_values = {'C': [1, 15, 30, 50]}  \n",
    "# # grid_values = {'C': [30]}\n",
    "# # 设定打分为roc_auc\n",
    "# \"\"\"\n",
    "# penalty: l1 or l2, 用于指定惩罚中使用的标准。\n",
    "# \"\"\"\n",
    "# model_LR = GridSearchCV(LR(penalty='l2', dual=True, random_state=0), grid_values, scoring='roc_auc', cv=20)\n",
    "# model_LR.fit(train_x, label)\n",
    "# # 20折交叉验证\n",
    "# # GridSearchCV(cv=20, \n",
    "# #         estimator=LR(C=1.0, \n",
    "# #             class_weight=None, \n",
    "# #             dual=True, \n",
    "# #             fit_intercept=True, \n",
    "# #             intercept_scaling=1, \n",
    "# #             penalty='l2', \n",
    "# #             random_state=0, \n",
    "# #             tol=0.0001),\n",
    "# #         fit_params={}, \n",
    "# #         iid=True,\n",
    "# #         n_jobs=1,\n",
    "# #         param_grid={'C': [30]}, \n",
    "# #         pre_dispatch='2*n_jobs',\n",
    "# #         refit=True,\n",
    "# #         scoring='roc_auc', \n",
    "# #         verbose=0)\n",
    "\n",
    "# # 输出结果\n",
    "# # print(model_LR.grid_scores_, '\\n', model_LR.best_params_, model_LR.best_params_)\n",
    "# print(model_LR.cv_results_, '\\n', model_LR.best_params_, model_LR.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'mean_fit_time': array([0.77368994, 1.95680232, 2.88316183, 3.50976259]), 'std_fit_time': array([0.05099312, 0.19345662, 0.39457327, 0.50422455]), 'mean_score_time': array([0.00273149, 0.0025926 , 0.00262785, 0.00249476]), 'std_score_time': array([0.0001698 , 0.00014623, 0.00014215, 0.00024111]), 'param_C': masked_array(data=[1, 15, 30, 50],\n",
    "#              mask=[False, False, False, False],\n",
    "#        fill_value='?',\n",
    "#             dtype=object), 'params': [{'C': 1}, {'C': 15}, {'C': 30}, {'C': 50}], \\\n",
    "#  'split0_test_score': array([0.95273728, 0.95990784, 0.960192  , 0.9602816 ]), \\\n",
    "#  'split1_test_score': array([0.96081408, 0.96953856, 0.96975104, 0.96994816]), \\\n",
    "#  'split2_test_score': array([0.9583616 , 0.96794112, 0.96825856, 0.96836352]), \\\n",
    "#  'split3_test_score': array([0.95249152, 0.96079104, 0.96123136, 0.96137984]), \\\n",
    "#  'split4_test_score': array([0.96460288, 0.9721088 , 0.9724672 , 0.97263104]), \\\n",
    "#  'split5_test_score': array([0.95881216, 0.96733184, 0.96779008, 0.96797184]), \\\n",
    "#  'split6_test_score': array([0.95679232, 0.96563968, 0.96596736, 0.96606976]), \\\n",
    "#  'split7_test_score': array([0.95171072, 0.96053248, 0.96105216, 0.96125952]), \\\n",
    "#  'split8_test_score': array([0.95526656, 0.9604096 , 0.96051712, 0.96053248]), \\\n",
    "#  'split9_test_score': array([0.94979328, 0.95777024, 0.95817472, 0.95834368]), \\\n",
    "#  'split10_test_score': array([0.95965952, 0.9672192 , 0.9675264 , 0.96764672]), \\\n",
    "#  'split11_test_score': array([0.95329024, 0.96009472, 0.96019712, 0.96021504]), \\\n",
    "#  'split12_test_score': array([0.96268544, 0.97140224, 0.97184256, 0.97202944]), \\\n",
    "#  'split13_test_score': array([0.9571968 , 0.96615936, 0.9666048 , 0.96676864]), \\\n",
    "#  'split14_test_score': array([0.95916544, 0.96551936, 0.96583168, 0.96596992]), 'split15_test_score': array([0.96279296, 0.96956928, 0.96978176, 0.96979968]), 'split16_test_score': array([0.95332096, 0.96132352, 0.96161792, 0.96173568]), 'split17_test_score': array([0.94883328, 0.9570816 , 0.95749632, 0.95771136]), 'split18_test_score': array([0.9528448 , 0.96074496, 0.96114176, 0.9612672 ]), 'split19_test_score': array([0.96429824, 0.97186048, 0.972032  , 0.97212416]), 'mean_test_score': array([0.9567735 , 0.9646473 , 0.9649737 , 0.96510246]), 'std_test_score': array([0.0046911 , 0.00476416, 0.00475249, 0.00475557]), 'rank_test_score': array([4, 3, 2, 1], dtype=int32), 'split0_train_score': array([0.99254593, 1.        , 1.        , 1.        ]), 'split1_train_score': array([0.99230078, 1.        , 1.        , 1.        ]), 'split2_train_score': array([0.9923811, 1.       , 1.       , 1.       ]), 'split3_train_score': array([0.9924227, 1.       , 1.       , 1.       ]), 'split4_train_score': array([0.9923401, 1.       , 1.       , 1.       ]), 'split5_train_score': array([0.9924475, 1.       , 1.       , 1.       ]), 'split6_train_score': array([0.99238184, 1.        , 1.        , 1.        ]), 'split7_train_score': array([0.99249388, 1.        , 1.        , 1.        ]), 'split8_train_score': array([0.99257082, 1.        , 1.        , 1.        ]), 'split9_train_score': array([0.99253744, 1.        , 1.        , 1.        ]), 'split10_train_score': array([0.99235201, 1.        , 1.        , 1.        ]), 'split11_train_score': array([0.99243953, 1.        , 1.        , 1.        ]), 'split12_train_score': array([0.99236668, 1.        , 1.        , 1.        ]), 'split13_train_score': array([0.99248181, 1.        , 1.        , 1.        ]), 'split14_train_score': array([0.99254685, 1.        , 1.        , 1.        ]), 'split15_train_score': array([0.99240575, 1.        , 1.        , 1.        ]), 'split16_train_score': array([0.99240521, 1.        , 1.        , 1.        ]), 'split17_train_score': array([0.99248037, 1.        , 1.        , 1.        ]), 'split18_train_score': array([0.99243375, 1.        , 1.        , 1.        ]), 'split19_train_score': array([0.99242053, 1.        , 1.        , 1.        ]), 'mean_train_score': array([0.99243773, 1.        , 1.        , 1.        ]), 'std_train_score': array([7.34564551e-05, 0.00000000e+00, 2.48253415e-17, 2.48253415e-17])} \n",
    "#  {'C': 50} 0.965102464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_LR = LR(penalty='l2', dual=True, random_state=0)\n",
    "# model_LR.fit(train_x, label)\n",
    "\n",
    "# test_predicted = np.array(model_LR.predict(test_x))\n",
    "# print('保存结果...')\n",
    "# submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "# print(submission_df.head(10))\n",
    "# submission_df.to_csv('/Users/jiangzl/Desktop/submission_br.csv',columns = ['id','sentiment'], index = False)\n",
    "# print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "# def review_to_wordlist(review, remove_stopwords=False):\n",
    "#     # review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "#     review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "\n",
    "#     words = review_text.lower().split()\n",
    "\n",
    "#     if remove_stopwords:\n",
    "#         stops = set(stopwords.words(\"english\"))\n",
    "#         words = [w for w in words if not w in stops]\n",
    "#     # print(words)\n",
    "#     return(words)\n",
    "\n",
    "\n",
    "# def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "#     '''\n",
    "#     1. 将评论文章，按照句子段落来切分(所以会比文章的数量多很多)\n",
    "#     2. 返回句子列表，每个句子由一堆词组成\n",
    "#     '''\n",
    "#     review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "#     # raw_sentences 句子段落集合\n",
    "#     raw_sentences = tokenizer.tokenize(review)\n",
    "#     # print(raw_sentences)\n",
    "    \n",
    "#     sentences = []\n",
    "#     for raw_sentence in raw_sentences:\n",
    "#         if len(raw_sentence) > 0:\n",
    "#             # 获取句子中的词列表\n",
    "#             sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "#     return sentences\n",
    "\n",
    "\n",
    "# sentences = []\n",
    "# for i, review in enumerate(train[\"review\"]):\n",
    "#     # print(i, review)\n",
    "#     sentences += review_to_sentences(review, tokenizer, True)\n",
    "    \n",
    "# unlabeled_train = pd.read_csv(\"%s/%s\" % (root_dir, \"unlabeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3 )\n",
    "# for review in unlabeled_train[\"review\"]:\n",
    "#     sentences += review_to_sentences(review, tokenizer)\n",
    "# print('预处理 unlabeled_train data...')\n",
    "\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "# # 模型参数\n",
    "# num_features = 300    # Word vector dimensionality                      \n",
    "# min_word_count = 40   # Minimum word count                        \n",
    "# num_workers = 4       # Number of threads to run in parallel\n",
    "# context = 10          # Context window size                                                                                    \n",
    "# downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# %%time\n",
    "# # 训练模型\n",
    "# print(\"训练模型中...\")\n",
    "# model = Word2Vec(sentences, workers=num_workers, \\\n",
    "#             size=num_features, min_count=min_word_count, \\\n",
    "#             window=context, sample=downsampling)\n",
    "# print(\"训练完成\")\n",
    "\n",
    "# print('保存模型...')\n",
    "# model.init_sims(replace=True)\n",
    "# model_name = \"%s/%s\" % (root_dir, \"300features_40minwords_10context\")\n",
    "# model.save(model_name)\n",
    "# print('保存结束')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaggle-nlp已经做过训练了，这里就直接把训练好的model拿来用\n",
    "model = Word2Vec.load(\"D:\\\\opt\\\\kaggle-nlp\\\\300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6180866956710815),\n",
       " ('lad', 0.597815215587616),\n",
       " ('lady', 0.5883153676986694),\n",
       " ('monk', 0.5241137742996216),\n",
       " ('person', 0.5198613405227661)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用W2V特征\n",
    "由于对段落中的所有词向量进行取平均操作已经在kaggle-nlp中实现过了，再次仅做后续补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    '''\n",
    "    对段落中的所有词向量进行取平均操作\n",
    "    '''\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "\n",
    "    # Index2word包含了词表中的所有词，为了检索速度，保存到set中\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    # 取平均\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    '''\n",
    "    给定一个文本列表，每个文本由一个词列表组成，返回每个文本的词向量平均值\n",
    "    '''\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "        if counter % 5000 == 0:\n",
    "            print(\"Review %d of %d\" % (counter, len(reviews)))\n",
    "\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time trainDataVecs = getAvgFeatureVecs(train_data, model, num_features)\n",
    "# print(np.shape(trainDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time testDataVecs = getAvgFeatureVecs(test_data, model, num_features)\n",
    "# print(np.shape(testDataVecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯贝叶斯+Word2vec训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB as GNB\n",
    "\n",
    "# model_GNB = GNB()\n",
    "# model_GNB.fit(trainDataVecs, label)\n",
    "\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "# import numpy as np\n",
    "\n",
    "# print(\"高斯贝叶斯分类器10折交叉验证得分: \", np.mean(cross_val_score(model_GNB, trainDataVecs, label, cv=10, scoring='roc_auc')))\n",
    "\n",
    "# print('保存结果...')\n",
    "# result = model_GNB.predict( testDataVecs )\n",
    "# submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': result})\n",
    "# print(submission_df.head(10))\n",
    "# submission_df.to_csv('/Users/jiangzl/Desktop/gnb_word2vec.csv',columns = ['id','sentiment'], index = False)\n",
    "# print('结束.')\n",
    "\n",
    "# \"\"\"\n",
    "# 从验证结果来看，没有超过基于TF-IDF多项式贝叶斯模型\n",
    "# \"\"\"\n",
    "\n",
    "# 高斯贝叶斯分类器10折交叉验证得分:  0.6163932159999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林+Word2vec训练\n",
    "此部分已在kaggle-nlp中出现过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# forest = RandomForestClassifier( n_estimators = 100, n_jobs=2)\n",
    "\n",
    "# print(\"Fitting a random forest to labeled training data...\")\n",
    "# %time forest = forest.fit( trainDataVecs, label )\n",
    "# print(\"随机森林分类器10折交叉验证得分: \", np.mean(cross_val_score(forest, trainDataVecs, label, cv=10, scoring='roc_auc')))\n",
    "\n",
    "# # 测试集\n",
    "# result = forest.predict( testDataVecs )\n",
    "\n",
    "# print('保存结果...')\n",
    "# submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': result})\n",
    "# print(submission_df.head(10))\n",
    "# submission_df.to_csv('/Users/jiangzl/Desktop/rf_word2vec.csv',columns = ['id','sentiment'], index = False)\n",
    "# print('结束.')\n",
    "\n",
    "# \"\"\"\n",
    "# 改用随机森林之后，效果有提升，但是依然没有超过基于TF-IDF多项式贝叶斯模型\n",
    "# \"\"\"\n",
    "# 随机森林分类器10折交叉验证得分:  0.6428176640000001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
